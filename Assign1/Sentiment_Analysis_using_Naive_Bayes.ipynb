{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_using_Naive_Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debgit/NLP/blob/main/Assign1/Sentiment_Analysis_using_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4tgyjYGU5mh"
      },
      "source": [
        "# Sentiment Analysis using Naive Bayes\n",
        "\n",
        "In this assignment, we will attempt to label tweets with sentiments (positive, neutral and negative) using Naive Bayes classifier. Naive Bayes is a very basic approach to this problem, but gives surprisingly good accuracy sometimes.\n",
        "\n",
        "**Fill in the Blanks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPyp7swyhfsk",
        "outputId": "aeebdb86-9c41-48b8-9138-6edb9808caef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mount_drive = True\n",
        "if mount_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af8UfnQOVXGZ"
      },
      "source": [
        "## Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91xo5PKAUoux"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEeXoKyvVqdQ"
      },
      "source": [
        "## Reading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "selIshNKkFTc"
      },
      "source": [
        "project_path='/content/gdrive/My Drive/NLP/'\n",
        "data_file = project_path+'tweets.csv'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "menn3WewVpe9",
        "outputId": "da201376-3434-4f7b-98f2-4ce50f1f73fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "data=pd.read_csv(data_file,encoding='UTF8')\n",
        "data.drop(data.columns[0],axis=1,inplace=True)\n",
        "data.sample(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>Barack Obama LONGBOARD Package CORE 7\" TRUCKS ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>One Direction Tolak Undangan Michelle Obama! h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>942</th>\n",
              "      <td>Concerning allegations about the firing of som...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408</th>\n",
              "      <td>RT @Talkmaster: Oh now I get it. Obama was tal...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Video shows federal officials joking about cos...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1021</th>\n",
              "      <td>#WhatsRomneyHiding his secret 'more flexible' ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>RT @WhatTheFFacts: In his teen years, Obama ha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1121</th>\n",
              "      <td>RT @wcptersn: If Obama were white, he'd be Mit...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1174</th>\n",
              "      <td>http://t.co/5XJZbGSV Naked Sarah Palin The pol...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>Obama to celebrate Passover with Seder http://...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tweets  labels\n",
              "54    Barack Obama LONGBOARD Package CORE 7\" TRUCKS ...       0\n",
              "314   One Direction Tolak Undangan Michelle Obama! h...       0\n",
              "942   Concerning allegations about the firing of som...       1\n",
              "408   RT @Talkmaster: Oh now I get it. Obama was tal...       1\n",
              "5     Video shows federal officials joking about cos...       0\n",
              "1021  #WhatsRomneyHiding his secret 'more flexible' ...       1\n",
              "515   RT @WhatTheFFacts: In his teen years, Obama ha...       0\n",
              "1121  RT @wcptersn: If Obama were white, he'd be Mit...       1\n",
              "1174  http://t.co/5XJZbGSV Naked Sarah Palin The pol...       0\n",
              "1244  Obama to celebrate Passover with Seder http://...       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3SXxjJOqgkG",
        "outputId": "8c554be3-085d-45b9-d082-c1d72a3ed8e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "data.isnull().sum()\n",
        "## there are 5 tweets which are null,should be dropped from the training"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tweets    5\n",
              "labels    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgcNrqs6szu2"
      },
      "source": [
        "data.dropna(inplace=True)\n",
        "data.reset_index(inplace=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bUNORaDVwrN"
      },
      "source": [
        "## Text processing for the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCtQLFwcHauQ",
        "outputId": "f401a1cf-15cd-4c1a-a2eb-f1e39fd9cfd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbVn9swJVuLA"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation \n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])\n",
        "    \n",
        "def processTweet(tweet):\n",
        "    # tweet is the text we will pass for preprocessing \n",
        "    # convert passed tweet to lower case \n",
        "    tweet=  tweet.lower()\n",
        "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
        "    tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
        "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
        "    \n",
        "    # use work_tokenize imported above to tokenize the tweet\n",
        "    tweet=word_tokenize(tweet)\n",
        "    return [word for word in tweet if len(word)>2 if word not in stopwords]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gk8veQrWK7J"
      },
      "source": [
        "## Process all tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44jBcZrTV1QQ"
      },
      "source": [
        "processed=[]\n",
        "\n",
        "for tweet in data['tweets']:\n",
        "    # process all tweets using processTweet function above - store in variable 'cleaned' \n",
        "    cleaned=processTweet(tweet)\n",
        "    processed.append(' '.join(cleaned))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ_2PZV-WO_E"
      },
      "source": [
        "data['processed'] = processed"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRLa5Xj8tynD",
        "outputId": "2008dc91-4a7b-4d64-f02c-823755f8ae4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "data.sample(10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweets</th>\n",
              "      <th>labels</th>\n",
              "      <th>processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>652</td>\n",
              "      <td>RT @LouTommoBum: Michelle Obama invited the bo...</td>\n",
              "      <td>0</td>\n",
              "      <td>michelle obama invited boys easter party l0l</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>978</td>\n",
              "      <td>@benhurleycomedy haha is the first person you ...</td>\n",
              "      <td>0</td>\n",
              "      <td>haha first person followed obama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1261</th>\n",
              "      <td>1265</td>\n",
              "      <td>Napolitano Caught Hiring Muslim Brotherhood Te...</td>\n",
              "      <td>1</td>\n",
              "      <td>napolitano caught hiring muslim brotherhood te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>Here's How Obama and the Democrats Will Win in...</td>\n",
              "      <td>1</td>\n",
              "      <td>obama democrats win 2012 let start going back ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>234</td>\n",
              "      <td>Interesting take: Obama Misquotes Bible on Wea...</td>\n",
              "      <td>1</td>\n",
              "      <td>interesting take obama misquotes bible wealth ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>510</td>\n",
              "      <td>I favorited a @YouTube video http://t.co/LdPff...</td>\n",
              "      <td>1</td>\n",
              "      <td>favorited video tales fantasy obama look</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>289</td>\n",
              "      <td>Obama says knock you out -- http://t.co/PUZRq7...</td>\n",
              "      <td>0</td>\n",
              "      <td>obama says knock screwytees</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1153</th>\n",
              "      <td>1156</td>\n",
              "      <td>RT @markknoller: Obama Campaign seizing on Was...</td>\n",
              "      <td>0</td>\n",
              "      <td>obama campaign seizing washington post report ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>561</th>\n",
              "      <td>563</td>\n",
              "      <td>Dick riding obama obama -thugnificent</td>\n",
              "      <td>0</td>\n",
              "      <td>dick riding obama obama -thugnificent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>744</td>\n",
              "      <td>@Devils_for_Life lol No Obama is :)</td>\n",
              "      <td>0</td>\n",
              "      <td>lol obama</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  ...                                          processed\n",
              "650     652  ...       michelle obama invited boys easter party l0l\n",
              "976     978  ...                   haha first person followed obama\n",
              "1261   1265  ...  napolitano caught hiring muslim brotherhood te...\n",
              "12       12  ...  obama democrats win 2012 let start going back ...\n",
              "234     234  ...  interesting take obama misquotes bible wealth ...\n",
              "510     510  ...           favorited video tales fantasy obama look\n",
              "289     289  ...                        obama says knock screwytees\n",
              "1153   1156  ...  obama campaign seizing washington post report ...\n",
              "561     563  ...              dick riding obama obama -thugnificent\n",
              "742     744  ...                                          lol obama\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KWUQNNdv4So"
      },
      "source": [
        "## from the above output it is noticed that not all the puntuations are removed hence there is a need to refine the function."
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JabmRdNiWUhc"
      },
      "source": [
        "## Create pipeline and define parameters for GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azZvCgLsWVaZ"
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MultinomialNB())])\n",
        "\n",
        "tuned_parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'tfidf__norm': ('l1', 'l2'),\n",
        "    'clf__alpha': [1, 1e-1, 1e-2]\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0xeqceWWbz_"
      },
      "source": [
        "## Split data into test and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uznVuAUUWbPM"
      },
      "source": [
        "# split data into train and test with split as 0.2 \n",
        "X = data['processed']\n",
        "Y = data['labels']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size =.2,random_state=7,stratify=data['labels'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHsawSmR6Hsd",
        "outputId": "ad99d126-7600-415e-f3a6-39b5793cee31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('X_train',X_train.shape)\n",
        "print('X_test',X_test.shape)\n",
        "print('Y_train',Y_train.shape)\n",
        "print('Y_test',Y_test.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (1100,)\n",
            "X_test (275,)\n",
            "Y_train (1100,)\n",
            "Y_test (275,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ZgcaM8WfRB"
      },
      "source": [
        "## Perform classification (using GridSearch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUwwb2IWWhmH",
        "outputId": "4f19b521-3210-410d-9582-253d7191e171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "# perform GridSearch CV with 10 fold CV using pipeline and tuned_paramters defined above \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "clf = GridSearchCV(text_clf,tuned_parameters,cv=10)\n",
        "clf.fit(X_train,Y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_accents=None,\n",
              "                                                        token_pattern='(?u...\n",
              "                                                         sublinear_tf=False,\n",
              "                                                         use_idf=True)),\n",
              "                                       ('clf',\n",
              "                                        MultinomialNB(alpha=1.0,\n",
              "                                                      class_prior=None,\n",
              "                                                      fit_prior=True))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'clf__alpha': [1, 0.1, 0.01],\n",
              "                         'tfidf__norm': ('l1', 'l2'),\n",
              "                         'tfidf__use_idf': (True, False),\n",
              "                         'vect__ngram_range': [(1, 1), (1, 2), (2, 2)]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s43xHo1u89y5",
        "outputId": "6548e64e-5cf2-4cb6-c356-d1efa9b2b54a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "clf.best_params_"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf__alpha': 0.1,\n",
              " 'tfidf__norm': 'l2',\n",
              " 'tfidf__use_idf': False,\n",
              " 'vect__ngram_range': (1, 2)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5so4j2q9EqL"
      },
      "source": [
        "Y_pred=clf.predict(X_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE_mfhiUWyc8"
      },
      "source": [
        "## Classification report "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqvkzGFRWzIb",
        "outputId": "1c079147-095b-45fc-bd8c-e89b49d761f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# print classification report after predicting on test set with best model obtained in GridSearch\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "Y_pred=clf.predict(X_test)\n",
        "print(classification_report(Y_test,Y_pred))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.97      0.92       188\n",
            "           1       0.88      0.72      0.79        71\n",
            "           2       1.00      0.38      0.55        16\n",
            "\n",
            "    accuracy                           0.87       275\n",
            "   macro avg       0.92      0.69      0.75       275\n",
            "weighted avg       0.88      0.87      0.86       275\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfm9wCgy99pz",
        "outputId": "82b134cc-651f-4bea-ff87-708a44baf118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(confusion_matrix(Y_test,Y_pred)) ## the recall is not good for class 1 and 2."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[183   5   0]\n",
            " [ 20  51   0]\n",
            " [  8   2   6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shTpptLeW1SF"
      },
      "source": [
        "## Important:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdWycpFYW3iD",
        "outputId": "26eccf3f-7c41-4ed3-e91b-f605e45304a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "counts = data.labels.value_counts()\n",
        "print(counts)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    942\n",
            "1    352\n",
            "2     81\n",
            "Name: labels, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtModyHuhPQQ",
        "outputId": "0ae7d854-3c16-49c7-d588-b86d49cfb6ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!pip install imblearn"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l3PcKXfW9LE"
      },
      "source": [
        "We can see above that the class distribution is highly imbalanced, this would not lead to good sampling of the data for the classifier. For your learning, try using [SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/api.html) to oversample the minority classes and then evaluate the performance with Naive Bayes and compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aY3Eg7EKGY6",
        "outputId": "a05196a1-8bcc-4c82-db07-bf473ad6e9ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from imblearn.over_sampling import SMOTE, SVMSMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5MyXET9RaUr"
      },
      "source": [
        "sampler = SMOTE(random_state=0)\n",
        "\n",
        "text_clf_sm = Pipeline([                        \n",
        "                      ('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('smot',SMOTE()),\n",
        "                     ('clf', MultinomialNB())\n",
        "                     ])\n",
        "\n",
        "#p1=Pipeline(sampler,text_clf_sm)\n",
        "\n",
        "tuned_parameters_sm = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'tfidf__norm': ('l1', 'l2'),\n",
        "    'smot__k_neighbors' : [5,10,15,20],\n",
        "    #'sampling_strategy':(str),\n",
        "    'clf__alpha': [1, 1e-1, 1e-2]\n",
        "}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW4Ks7uoR5Os",
        "outputId": "caeb86cd-1fe5-45c1-ef83-633aa97b4903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "clf_sm = GridSearchCV(text_clf_sm,tuned_parameters_sm,cv=10)\n",
        "clf_sm.fit(X_train,Y_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_accents=None,\n",
              "                                                        token_pattern='(?u...\n",
              "                                        MultinomialNB(alpha=1.0,\n",
              "                                                      class_prior=None,\n",
              "                                                      fit_prior=True))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'clf__alpha': [1, 0.1, 0.01],\n",
              "                         'smot__k_neighbors': [5, 10, 15, 20],\n",
              "                         'tfidf__norm': ('l1', 'l2'),\n",
              "                         'tfidf__use_idf': (True, False),\n",
              "                         'vect__ngram_range': [(1, 1), (1, 2), (2, 2)]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdLSBT9FaUGt",
        "outputId": "407f34d6-3db4-4442-9fd2-a9d9e4e697b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "clf_sm.best_params_"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf__alpha': 0.01,\n",
              " 'smot__k_neighbors': 20,\n",
              " 'tfidf__norm': 'l1',\n",
              " 'tfidf__use_idf': True,\n",
              " 'vect__ngram_range': (2, 2)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SVdIHqTaGHO",
        "outputId": "3e51ed3a-6d4c-496f-c101-b3ddff9a5595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "## We can see now that the recall value has increased for class 1 and 2.\n",
        "Y_pred=clf_sm.predict(X_test)\n",
        "print(classification_report(Y_test,Y_pred))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88       188\n",
            "           1       0.80      0.68      0.73        71\n",
            "           2       0.33      0.38      0.35        16\n",
            "\n",
            "    accuracy                           0.81       275\n",
            "   macro avg       0.67      0.65      0.66       275\n",
            "weighted avg       0.82      0.81      0.81       275\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuWFkbvEhPRS"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDhhVPRhhPRV"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}